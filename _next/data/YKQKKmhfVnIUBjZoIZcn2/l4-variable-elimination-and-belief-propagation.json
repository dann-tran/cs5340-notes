{"pageProps":{"metadataList":[{"slug":"l2-bayesian-networks","title":"Bayesian Networks (Directed Graphical Models)","tag":"representation","lectureNumber":2},{"slug":"l3-mrf","title":"Markov Random Fields (Undirected Graphical Models)","tag":"representation","lectureNumber":3},{"slug":"l4-variable-elimination-and-belief-propagation","title":"Variable Elimination and Belief Propogation","tag":"exact inference","lectureNumber":4},{"slug":"l5-factor-graph-and-jt-algo","title":"Factor Graph and Junction Tree Algorithm","tag":"exact inference","lectureNumber":5},{"slug":"l6-parameter-learning-with-complete-data","title":"Parameter Learning with Complete Data","tag":"learning","lectureNumber":6},{"slug":"l7-mixture-models-and-em-algo","title":"Mixture Models and the EM Algorithm","tag":"learning","lectureNumber":7},{"slug":"l8-hmm","title":"Hidden Markov Models (HMMs)","tag":"modelling","lectureNumber":8},{"slug":"l9-monte-carlo-inference","title":"Monte Carlo Inference (Sampling)","tag":"approximate inference","lectureNumber":9},{"slug":"l10-variational-inference","title":"Variational Inference","tag":"approximate inference","lectureNumber":10},{"slug":"l11-variational-autoencoder-and-mixture-density-networks","title":"Variational Autoencoder and Mixture Density Networks","tag":"modelling","lectureNumber":11},{"slug":"l12-graph-cut-and-alpha-expansion","title":"Graph-Cut and Alpha-Expansion","tag":"modelling","lectureNumber":12}],"post":{"metadata":{"slug":"l4-variable-elimination-and-belief-propagation","title":"Variable Elimination and Belief Propogation","tag":"exact inference","lectureNumber":4},"markdownBody":"\n## Probabilistic inference\n\nProbabilistic inference problem: Let $E$ and $F$ be disjoint subsets of the node indices of a graphical model, s.t. $X_E$ (evidence nodes) and $X_F$ (query nodes) are disjoint subsets of r.v. in the domain. Our goal is to calculate $p(x_F|x_E)$.\n\nSuppose that $V=E\\cup F\\cup R$, then we want to compute\n\n$$p(x_F|x_E)=\\frac{p(x_E, x_F)}{p(x_E)}=\\frac{\\sum_{x_R}p(x)}{\\sum_{x_F}p(x_E, x_F)}$$\n\nA na√Øve summation over the joint distribution of $m$ variables that take $k$ states will incur a computational complexity of $O(k^n)$. To reduce computational complexity, we can exploit the factorization of the joint probability.\n\nLet $X_i$ be an evidence node whose observed value is $\\bar x_i$. Define an evidence potential $\\delta(x_i, \\bar x_i)=\\mathbb I(x_i=\\bar x_i)$. Then $g(\\bar x_i)=\\sum_{x_i}g(x_i)\\delta(x_i, \\bar x_i)$.\n\nTotal evidence potential: $\\delta(x_E, \\bar x_E)\\triangleq\\prod_{i\\in E}\\delta(x_i, \\bar x_i)$. Thus\n\n$$\n\\begin{align*}\np(x_F, \\bar x_E)&=\\sum_{x_E}p(x_F, x_E)\\delta(x_E, \\bar x_E) \\\\\np(\\bar x_E)&=\\sum_{x_F}\\sum_{x_E}p(x_F, x_E)\\delta(x_E, \\bar x_E)\n\\end{align*}\n$$\n\nThis suggests that it may be useful to define a generalized measure that represents conditional probability w.r.t. $E$:\n\n$$\np^E(x)\\triangleq p(x)\\delta(x_E, \\bar x_E)\n$$\n\nBy formally \"marginalizing\" this measure w.r.t. $x_E$, we evaluate $p(x)$ at $X_E=\\bar x_E$, and obtain $p(x_F, \\bar x_E)$, an unnormalized version of the conditional probability $p(x_F|\\bar x_E)$. I.e. $p(x_F, \\bar x_E)=\\sum_{x_E}p^E(x)$.\n\nThis tactic is particularly natural in the case of of UDMs, where multiplication by an evidence potential $\\delta(x_i, \\bar x_i)$ can be implemented by simiply redefining the local potentials $\\psi(x_i)$ for $i\\in E$. Thus we define $\\psi_i^E(x_i)\\triangleq\\psi_i(x_i)\\delta(x_i, \\bar x_i)$ for $i\\in E$. Leaving all other clique potentials unchanged i.e. $\\psi_C^E(x_C)\\triangleq\\psi_C(x_C)$ for $C\\notin\\{\\{i\\}:i\\in E\\}$, we obtain the desired unnormalized representation:\n\n$$\np^E(x)\\triangleq\\frac 1 Z\\prod_{C\\in\\mathcal C}\\psi_C^E(x_C)\n$$\n\n### Elimination and directed graphs\n\n---\n\n**ELIMINATE algo for probabilistic inference on directed graphs**\n\n$\\text{Eliminate}(G, E, F)$:\n\n1. $\\text{Initialize}(G, F)$\n2. $\\text{Evidence}(E)$\n3. $\\text{Update}(G)$\n4. $\\text{Normalize}(F)$\n\n$\\text{Initialize}(G, F)$:\n\n1. Choose an ordering $I$ s.t. $F$ appears last.\n2. for each node $X_i$ in V, place $p(x_i|x_{\\pi_i})$ on the active list.\n\n$\\text{Evidence}(E)$: for each $i$ in $E$, place $\\delta(x_i, \\bar x_i)$ on the active list.\n\n$\\text{Update}(G)$: for each $i$ in $I$\n\n1. Find all potentials from the active list that reference $x_i$ and remove them from the active list.\n2. Let $\\phi_i(x_{T_i})$ denote the product of these potentials.\n3. Let $m_i(x_{S_i})=\\sum_{x_i}\\phi_i(x_{T_i})$.\n4. Place $m_i(x_{S_i})$ on the active list.\n\n$\\text{Normalize}(F)$: $p(x_F|\\bar x_E)\\leftarrow \\phi_F(x_F)/\\sum_{x_F}\\phi_F(x_F)$\n\n---\n\nExample:\n![A DGM](dgm.png)\n\n$$\n\\begin{align*}\np(x_1, \\bar x_6)&=\\sum_{x_2}\\sum_{x_3}\\sum_{x_4}\\sum_{x_5}p(x_1)p(x_2|x_1)p(x_3|x_1)p(x_4|x_2)p(x_5|x_3)p(\\bar x_6|x_2, x_5) \\\\\n&=p(x_1)\\sum_{x_2}p(x_2|x_1)\\sum_{x_3}p(x_3|x_1)\\sum_{x_4}p(x_4|x_2)\\sum_{x_5}p(x_5|x_3)p(\\bar x_6|x_2, x_5) \\\\\n&=p(x_1)\\sum_{x_2}p(x_2|x_1)\\sum_{x_3}p(x_3|x_1)\\sum_{x_4}p(x_4|x_2)m_5(x_2, x_3) \\\\\n&=p(x_1)\\sum_{x_2}p(x_2|x_1)\\sum_{x_3}p(x_3|x_1)m_5(x_2, x_3)\\sum_{x_4}p(x_4|x_2) \\\\\n&=p(x_1)\\sum_{x_2}p(x_2|x_1)m_4(x_2)\\sum_{x_3}p(x_3|x_1)m_5(x_2, x_3) \\\\\n&=p(x_1)\\sum_{x_2}p(x_2|x_1)m_4(x_2)m_3(x_1, x_2) \\\\\n&=p(x_1)m_2(x_1)\n\\end{align*}\n$$\n\nwhere $m_i(x_j)=\\sum_{x_i}f(x_j)$. We can then calculate the inference:\n\n$$\np(x_1|\\bar x_6)\\frac{p(x_1)m_2(x_1)}{\\sum_{x_1}p(x_1)m_2(x_1)}\n$$\n\n### Elimination and undirected graphs\n\nThe same perspective of DGMs applies to UGMs, and the entire $\\text{Eliminate}$ algo goes through without essential change to the undireted case, except for the $\\text{Initialize}$ procedure where instead of using local probabilites we initialzie the active list to contain the potentials $\\{\\psi_C(x_C)\\}$.\n\nWhen calculating conditonal probabilities, the normalization factor $Z$ cancels:\n\n$$\np(x_F|x_E)=\\frac{\\frac 1 Z m_{E, R}(x_F)}{\\sum_{x_F}\\frac 1 Z m_{E, R}(x_F)}=\\frac{ m_{E, R}(x_F)}{\\sum_{x_F}m_{E, R}(x_F)}\n$$\n\nBut for a marginal probability $Z$ does not cancel and must be calculated explicitly.\n\n## Graph elimination\n\n---\n\n**A simple greedy algo for eliminating nodes in an undirected graph $G$**\n\n$\\text{UndirectedGraphEliminate}(G, I)$: for each node $X_i$ in $I$\n\n1. Connect all of the remaining neighbours of $X_i$.\n2. Remove $X_i$ from the graph.\n\n---\n\nReconstuted graph: the graph $\\tilde G=(V, \\tilde E)$, whose edge set $\\tilde E$ is a superset of $E$, incorporating all of the original edges $E$, as well as any new edges created during a run of $\\text{UndirectedGraphEliminate}$.\n\nThe elimination process adds new edges between (remaining) neighbours of the node. This creates new elimination cliques in the graph, and the overall complexity depends on the size of the largest elimination clique, which depends on the choice of elimination ordering.\n\n---\n\n**An algo for eliminating nodes in a directed graph $G$**\n\n$\\text{DirectedGraphEliminate}(G, I)$:\n\n1. $G^m=\\text{Moralize}(G)$\n2. $\\text{UndirectedGraphEliminate}(G^m, I)$\n\n$\\text{Moralize}(G)$:\n\n1. For each node $X_i$ in $I$, connect all of the parents of $X_i$.\n2. Drop the orientation of all edges.\n3. Return $G$.\n\n---\n\nTo define the elimination cliques for a directed graph, run $\\text{DirectedGraphEliminate}$.\n\n## Probabilistic inference on trees\n\nDefinitions:\n\n- In the undirected case: a tree is an undirected graph in which there is one and only one path between any pair of nodes.\n- In the directed case, a tree is any graph whose moralized graph is an undirected tree.\n\nAny undirected tree can be converted into a directed tree by choosing a root node and orienting all edges to point away from the root. From the POV of graphcial model representations, a directed tree and the corresponding undirected tree make exactly the same set of CI assertions.\n\n### Parameterization and conditioning\n\nLet us consider the parameterization of probability distributions on undirected trees. The cliques are single nodes and pairs of nodes, and thus the joint probability can be parameterized via potential functions $\\{\\psi(x_i)\\}$ and $\\{\\psi(x_i, x_j)\\}$. In particular, for a tree $T(V, E)$ we have:\n\n$$\np(x)=\\frac 1 Z\\left(\\prod_{i\\in V}\\psi(x_i)\\prod_{(i, j)\\in E}\\psi(x_i, x_j)\\right)\n$$\n\nFor a directed tree, the joint probability is formed by taking a product over a marginal probability $p(x_r)$ at the root node $r$, and the conditional probabilities $\\{p(x_j|x_i)\\}$ at all other nodes:\n\n$$\np(x)=p(x_r)\\prod_{(i, j)\\in E}p(x_j|x_i)\n$$\n\nwhere $(i, j)$ is a directed edge s.t. $i$ is the (uniqe) parent of $j$ i.e. $\\{i\\}=\\pi_j$.\n\nWe treat the parameterization for a directed tree a special case of that for an undirected tree i.e.\n\n- $\\psi(x_r)=p(x_r)$\n- $\\psi(x_i, x_j)=p(x_j|x_i)$ for $i$ is the parent of $j$\n- $\\psi(x_i)=1$ for $i\\neq r$\n\nWe use evidence potentials to capture conditioning. Specifically,\n\n$$\n\\psi_i^E(x_i)\\triangleq\n\\begin{cases}\n\\psi_i(x_i)\\delta(x_i, \\bar x_i) &i\\in E \\\\\n\\psi_i(x_i) & i\\notin E\n\\end{cases}\n$$\n\n$$\np(x|\\bar x_E) = \\frac 1{Z^E}\\left(\\prod_{i\\in V}\\psi^E(x_i)\\prod_{(i, j)\\in E}\\psi(x_i, x_j)\\right)\n$$\n\nThe parameterization of unconditional distributions and conditional distributions on trees is formally identical, involving a product of potential functions associated with each node and each edge in the graph. We can thus proceed without making any special distinction between the unconditional case and the conditional case.\n\n### Message passing\n\nThe basic structure of $\\text{Eliminate}$:\n\n- (1) Choose an elimination ordering $I$ in which the query node $f$ is the final node.\n- (2) Place all potentials on an active list.\n- (3) Eliminate a node $i$ by removing all potentials referencing the node from the active list, taking the product, summing over $x_i$ and placing the resulting intermediate factor back on the list.\n\n(1) To take the advantage of the recursive sturcture of a tree, we specify an elimination ordering $I$ that arises from a depth-first traversal of the tree and in which a node is eliminated only after all of its children in the directed version of the tree are eliminated. It can be easily everified that such an eliminiation ordering proceeds inward from the leaves, and generates elimination cliques of size at most two. This implies that the tree-width of a tree is equal to one.\n\n(3) Consider neighbouring nodes $i$ and $j$ where $i$ is closer to the root than $j$. We are interested in the intermediate factor careted when $j$ is eliminated. We can show that the intermediate factor created by the sum over $x_j$ is a function solely of $x_i$, denoted as $m_{ji}(x_i)$. This is because none of the potentials in the product can reference any variable in the subtree below $j$. We then just need to consider the potentials that reference $x_j$:\n\n- $\\psi^E(x_j)$\n- $\\psi(x_i, x_j)$\n- $\\prod_{k\\in\\mathcal N(j)\\setminus i}m_{kj}(x_j)$: messages folowing towards $j$\n\n### The sum-product algo\n\nWe can obtain all marginals by simply doubling the amount of work required to compute a single marginal by passing messages inward from leaves to root then from root to leaves again. A single message will flow in both directions along each edge.\n\nMessage-Passing Protocol: A node can send a message to a neighbouring node when (and only when) it has received messages from all of its other nieghbours.\n\n---\n\n**The $\\text{Sum-Product}$ algo**\n\n$\\text{Sum-Product}(T, E)$:\n\n1. $\\text{Evidence}(E)$\n1. $f=\\text{ChooseRoot}(V)$\n1. For $e\\in\\mathcal N(f)$: $\\text{Collect}(f, e)$.\n1. For $e\\in\\mathcal N(f)$: $\\text{Distribute}(f, e)$.\n1. For $i\\in\\mathcal V$: $\\text{ComputeMarginal}(i)$.\n\n$\\text{Evidence}(E)$:\n\n1. For $i\\in E$: $\\psi^E(x_i)=\\psi(x_i)\\delta(x_i, \\bar x_i)$.\n1. For $i\\notin E$: $\\psi^E(x_i)=\\psi(x_i)$.\n\n$\\text{Collect}(i, j)$:\n\n1. For $k\\in \\mathcal N(j)\\setminus i$: $\\text{Collect}(j, k)$.\n1. $\\text{SendMessage}(j, i)$\n\n$\\text{Distribute}(i, j)$:\n\n1. $\\text{SendMessage}(i, j)$\n1. For $k\\in \\mathcal N(j)\\setminus i$: $\\text{Distribute}(j, k)$.\n\n$\\text{SendMessage}(j, i)$:\n\n$$\nm_{ji}(x_i)=\\sum_{x_j}\\left(\\psi^E(x_j)\\psi(x_i, x_j)\\prod_{k\\in\\mathcal N(j)\\setminus i}m_{kj}(x_j)\\right)\n$$\n\n$\\text{ComputeMarginal}(i)$\n\n$$\np(x_i)\\propto\\psi^E(x_i)\\prod_{j\\in\\mathcal N(i)}m_{ji}(x_i)\n$$\n\n---\n\n![Diagram for sum-product algo](sum-product.png)\n\n## Reference materials\n\n- Jordan, M. I. (2003). Chapter 3: The Elimination Algorithm. In _An Introduction to Probabilistic Graphical Models_.\n- Jordan, M. I. (2003). Chapter 4: Probability Propagation and Factor Graphs. In _An Introduction to Probabilistic Graphical Models_.\n"}},"__N_SSG":true}