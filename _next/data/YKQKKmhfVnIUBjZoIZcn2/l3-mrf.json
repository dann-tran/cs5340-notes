{"pageProps":{"metadataList":[{"slug":"l2-bayesian-networks","title":"Bayesian Networks (Directed Graphical Models)","tag":"representation","lectureNumber":2},{"slug":"l3-mrf","title":"Markov Random Fields (Undirected Graphical Models)","tag":"representation","lectureNumber":3},{"slug":"l4-variable-elimination-and-belief-propagation","title":"Variable Elimination and Belief Propogation","tag":"exact inference","lectureNumber":4},{"slug":"l5-factor-graph-and-jt-algo","title":"Factor Graph and Junction Tree Algorithm","tag":"exact inference","lectureNumber":5},{"slug":"l6-parameter-learning-with-complete-data","title":"Parameter Learning with Complete Data","tag":"learning","lectureNumber":6},{"slug":"l7-mixture-models-and-em-algo","title":"Mixture Models and the EM Algorithm","tag":"learning","lectureNumber":7},{"slug":"l8-hmm","title":"Hidden Markov Models (HMMs)","tag":"modelling","lectureNumber":8},{"slug":"l9-monte-carlo-inference","title":"Monte Carlo Inference (Sampling)","tag":"approximate inference","lectureNumber":9},{"slug":"l10-variational-inference","title":"Variational Inference","tag":"approximate inference","lectureNumber":10},{"slug":"l11-variational-autoencoder-and-mixture-density-networks","title":"Variational Autoencoder and Mixture Density Networks","tag":"modelling","lectureNumber":11},{"slug":"l12-graph-cut-and-alpha-expansion","title":"Graph-Cut and Alpha-Expansion","tag":"modelling","lectureNumber":12}],"post":{"metadata":{"slug":"l3-mrf","title":"Markov Random Fields (Undirected Graphical Models)","tag":"representation","lectureNumber":3},"markdownBody":"\n## CI properties of UGMs\n\n### Markov properties\n\nGlobal Markov property: $X_A$ is independent of $X_C$ given $X_B$ if the set of nodes $X_B$ separates the does $X_A$ from the nodes $X_C$ in the sense of naïve graph-theoretic separation. Thus, if every path from a node in $X_A$ to a node in $X_C$ includes at least one node in $X_B$, we assert that $X_A\\perp X_C\\vert X_B$ holds.\n\nAdditionally, if a set of observed variables form a cut-set between two halves of the graph, then variables in one half are independent from one in the other.\n\n![A cut-set between $X_A$ and $X_C$ when $X_B$ is observed](cutset.png)\n\nLocal Markov property: A node's MB is its set of immediate neighbours.\n\nPairwise Markov property: Two nodes are conditionally independent given the rest if there is no direct edge between them.\n\nIt is obvious that global Markov implies local Markov which implies pairwise Markov. It can be proven that, assuming $p$ is a positive density, pairwise implies global, and hence all the Markov properties are equivalent.\n\n### Determining CIs for a DGM using a UGM\n\nMoralization is the process of converting a DGM to a UGM by adding edges between the unmarried parents of a node and then dropping the orientation of the edges. This is to express the correct CI from the v-structure $A\\rightarrow B\\leftarrow C$.\n\nHowever, moralization can yield a fully connected directed graph and loses some CI information, thus we cannot use the moralized UGM to determine CI properties of the DGM. We can minimize this by first constructing the ancestral graph of DAG $G$ w.r.t. $U=A\\cup B\\cup C$ i.e. we remove all nodes from $G$ that are not in $U$ or are not ancestors of $U$; then we moralize this ancestral graph and apply the simple graph separation rules for UGMs.\n\n### Comparative semantics\n\nIt is not possible to reduce undirected models to directed models or vice versa. E.g.\n\n![(a) An undirected graph whose CI semantics cannot be captured by a directed graph on the same nodes. (b) A directed graph whose CI semantics cannot be captured by an undirected graph on the same nodes.](graph-representation-comparative-semantics.png)\n\nDGMs and UGMs are perfect maps for different sets of distributions, so neither is more powerful than th other as a representation.\n\n- No UGM can precisely represent all and only the two CI statements encoded by a v-structure. In general, CI properties in UGMs are monotomic i.e. if $A\\perp B\\vert C$ then $A\\perp B\\vert (C\\cup D)$.\n- In DGMs, CI properties can be non-monotonic, since conditioning on extra variables can eliminate CI due to explaining away.\n\nSome distributions can be perfectly modeled by either a DGM or a UGM; the resulting graphs are called decomposable or chordal. Roughly speaking, if we collapse together all the variables in each maximal clique, to make “mega-variables”, the resulting graph will be a tree.\n\n![Diagram of the distributions that DGMs and UGMs can represent](distribution-model-diagram.png)\n\n## Parameterization\n\nA clique of a graph is a fully-connected subset of nodes. The maximal cliques of a graph are the cliques that cannot be extended to include additional nodes without losing the property of being fully connected. Given that all cliques are subsets of one or more maximal cliques, we can restrict ourselves to maximal cliques WLOG, and the meaning of \"local\" for UDG should be \"maximal clqiue.\" More precisely, the CI properties of UDGs imply a representation of the joint probability as a product of local functions defined on the maximal cliques of the graph.\n\nLet $C$ be a set of indices of a maximal clique in an undirected graph $G$, and let $\\mathcal C$ be the set of all such $C$. A potential function (aka factor) $\\psi_C(x_C)$ is a function on the possible realizations $x_C$ of the maximal clique $X_C$. Potential functions are assumed to be nonnegative, real-valued functions, but are other artbitrary.\n\n### Hammersley-Clifford Theorem\n\nHammersley-Clifford Theorem: A positive distribution $p(x)>0$ satisfies the CI properties of an undirected graph $G$ iff $p$ can be represented as a product of factors, one per maximal clique i.e.\n\n$$\np(x)\\triangleq\\frac 1 Z\\prod_{C\\in\\mathcal C}\\psi_C(x_C)\n$$\n\nwhere $Z$ is the normalization factor\n\n$$\nZ\\triangleq\\sum_x\\prod_{C\\in\\mathcal C}\\psi_C(x_C)\n$$\n\n### Facts about parameterizations\n\nParameterization of MRFs is not unique. We a free to relax the parameterization to the edges of the graph, rather than the maximal cliques. This is pairwise MRF, and is widely used due to its simplicity, although it is not as general.\n\nCanonical paramterization of MRFs defines the parameterization over all cliques in the graph. A uniform prior can be assumed on any potential function.\n\n### Statistical physics interpretation\n\nThe basic idea is that a potential function favours certain local configurations of variables by assigning them a larger value. The global configurations that have a high probability are, roughly, those that satisfy as many of the vaoured local configurations as possible.\n\n$$\n\\begin{align*}\n\\psi_C(x_C)&=\\exp\\{-H_C(x_C)\\} \\\\\np(x)&=\\frac 1 Z\\prod_{C\\in\\mathcal C}\\exp\\{-H_C(x_C)\\} \\\\\n&=\\frac 1 Z\\exp\\left\\{-\\sum_{C\\in\\mathcal C}H_C(x_C)\\right\\} \\\\\n&=\\frac 1 Z \\exp\\{-H(x)\\}\n\\end{align*}\n$$\n\nwhere we have defined $H(x)\\triangleq\\sum_{C\\in\\mathcal C}H_C(x_C)$. We have represented the joint probability of a UDG model as a Boltzman/Gibbs distribution.\n\n### Log-linear model\n\nDefine the log-potentials as a linear function of the parameters\n\n$$\n\\log\\psi_C(x_C)\\triangleq \\bm\\phi_C(x_C)^T\\bm\\theta_C\n$$\n\nwhere $\\phi_C(x_C)$ is a feature vector derived from the values of the variables $x_C$.\nThe resulting log-probability has the form of a maximum entroy or a log-linear momdel\n\n$$\n\\log p(x)=\\sum_C\\bm\\phi_C(x_C)^T\\bm\\theta_C-Z\n$$\n\n### Advantages and disadvantages of MRFs\n\nAdvantages of MRFs:\n\n- They can be applied to a wider range of problems in which there is no natural directionality associated with variable dependencies.\n- Undirected graphs can succinctly express certain dependencies that Bayesian nets cannot easily describe (although the converse is also true)\n\nDrawbacks of MRFs:\n\n- Computing the normalization constant $Z$ requires summing over a potentially exponential number of assignments. We will see that in the general case, this will be NP-hard; thus many undirected models will be intractable and will require approximation techniques.\n- Undirected models may be difficult to interpret.\n- It is much easier to generate data from a Bayesian network, which is important in some applications.\n\n## Examples of MRFs\n\n### Ising model\n\n### Potts model\n\n## Conditional random fields (CRFs)\n\nA CRF, sometimes a discriminative random field, is a version of a MRF where all the clique potentials are conditioned on input features:\n\n$$\np(y| x)=\\frac 1 {Z(x)}\\prod_C\\psi_C(y_C|x_C)\n$$\n\nA CRF can be thought of as a structured output extension of logistic regression. Note that the partition function now depends on $x$, and $p(y|x)$ is a probability over $y$ and parameterized by $x$. In that sense, a CRF results in an instantiation of a new MRF for each input $x$.\n\nIn most practical applications, we further assume that the factors $\\psi_C(x_C, y_C)$ are of the form\n\n$$\n\\psi_C(y_C|x_C)=\\exp(\\mathbf w_c^T\\bm\\phi(x_C, y_C))\n$$\n\nwhere $\\mathbf w_c$ are parameters.\n\n### Advantages and disadvantages of CRFs\n\nAdvantages:\n\n- Modelling $p(x, y)$ using an MRF (viewed as a single model over $x$, $y$ with normalizing constant $Z=\\sum_{x, y}\\tilde p(x, y)$) requires fitting two distributions to the data: $p(y|x)$ and $p(x)$. However, if all we are interested in is predicting $y$ given $x$, then modelling $p(x)$ is unnecessary.\n- We can make the potentials (or factors) of the model data-independent.\n\nDisadvantages: CRFs require labelled training data and are slower to train.\n\n### Sequence labelling tasks\n\nThe most widely used kind of CRF uses a chain-structured graph to model correlation amongst neighboring labels. Such models are useful for a variety of sequence labeling tasks.\n\nTraditionally, HMMs have been used for such tasks, but an HMM requires specifying a generatie observation model $p(\\mathbf x_t|y_t, \\mathbf w)$ which can be difficult. Furthermore, each $\\mathbf x_t$ is required to be local, since it is hard to define a generative model for the whole stream of observations $\\mathbf x_{1:T}$\n\n$$\np(\\mathbf x, \\mathbf y|\\mathbf w)=\\prod_{t=1}^T p(y_t|y_{t-1}, \\mathbf w)p(\\mathbf x_t|y_t, \\mathbf w)\n$$\n\nAn obvious wway to make a discriminative version of an HMM is to reverse the arrows from $y_t$ to $\\mathbf x=\\mathbf x_t$. This defines a directed discriminative model called maximum entropy Markov model (MEMM)\n\n$$\np(\\mathbf y|\\mathbf x, \\mathbf w)=\\prod_t p(y_t|y_{t-1}, \\mathbf x, \\mathbf w)\n$$\n\nwhere $\\mathbf x=(\\mathbf x_{1:T}, \\mathbf x_g)$. An MEMM is simpy a Markov chain in which the state transition probabilities are conditioned on the input features.\n\nThis model suffers from the _label bias_ problem: local features at time $t$ do not influence states prior to time $t$. This follows by examining the DAG, which shows that $\\mathbf x_t$ is $d$-separated from $y_{t−1}$ (and all earlier time points) by the v-structure at $y_t$, which is a hidden child, thus blocking the information flow.\n\nE.g. POS tagging task \"banks\" in \"he banks at BoA\" and \"the river banks were overflowing.\"\n\nA chain-structured CRF model has the form\n\n$$\np(\\mathbf y|\\mathbf x, \\mathbf w)=\\frac 1{Z(\\mathbf x, \\mathbf w)}\\prod_{t=1}^T\\psi(y_t|\\mathbf x, \\mathbf w)\\prod_{t=1}^{T-1}\\psi(y_t, y_{t+1}|\\mathbf x, \\mathbf w)\n$$\n\nand the label bias problem no longer exists because $y_t$ does not block information from $\\mathbf x_t$ from reaching other $y_t$ nodes.\n\n## Reference materials\n\n- Jordan, M. I. (2003). Chapter 2: Conditional Independence and Factorization. In _An Introduction to Probabilistic Graphical Models_.\n- Murphy, K. P. (2012). Chapter 19: Undirected graphcial models (Markov random fields). In _Machine Learning: A Probabilistic Perspective_. The MIT Press.\n- Kuleshov, V. and Ermon, S. (2020). _Markov random fields_. CS228 notes. https://ermongroup.github.io/cs228-notes/representation/undirected/\n"}},"__N_SSG":true}