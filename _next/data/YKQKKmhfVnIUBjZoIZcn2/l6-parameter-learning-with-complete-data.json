{"pageProps":{"metadataList":[{"slug":"l2-bayesian-networks","title":"Bayesian Networks (Directed Graphical Models)","tag":"representation","lectureNumber":2},{"slug":"l3-mrf","title":"Markov Random Fields (Undirected Graphical Models)","tag":"representation","lectureNumber":3},{"slug":"l4-variable-elimination-and-belief-propagation","title":"Variable Elimination and Belief Propogation","tag":"exact inference","lectureNumber":4},{"slug":"l5-factor-graph-and-jt-algo","title":"Factor Graph and Junction Tree Algorithm","tag":"exact inference","lectureNumber":5},{"slug":"l6-parameter-learning-with-complete-data","title":"Parameter Learning with Complete Data","tag":"learning","lectureNumber":6},{"slug":"l7-mixture-models-and-em-algo","title":"Mixture Models and the EM Algorithm","tag":"learning","lectureNumber":7},{"slug":"l8-hmm","title":"Hidden Markov Models (HMMs)","tag":"modelling","lectureNumber":8},{"slug":"l9-monte-carlo-inference","title":"Monte Carlo Inference (Sampling)","tag":"approximate inference","lectureNumber":9},{"slug":"l10-variational-inference","title":"Variational Inference","tag":"approximate inference","lectureNumber":10},{"slug":"l11-variational-autoencoder-and-mixture-density-networks","title":"Variational Autoencoder and Mixture Density Networks","tag":"modelling","lectureNumber":11},{"slug":"l12-graph-cut-and-alpha-expansion","title":"Graph-Cut and Alpha-Expansion","tag":"modelling","lectureNumber":12}],"post":{"metadata":{"slug":"l6-parameter-learning-with-complete-data","title":"Parameter Learning with Complete Data","tag":"learning","lectureNumber":6},"markdownBody":"\n## Learning methods\n\n### Maximum likelihood estimate (MLE)\n\n$$\n\\begin{align*}\n\\hat\\theta\n&\\triangleq \\argmax_{\\theta}p(\\mathcal D|\\theta) \\\\\n&= \\argmax_{\\theta}\\log p(\\mathcal D|\\theta)\n\\end{align*}\n$$\n\n### Maximum a posteriori (MAP) estiamte\n\n$$\n\\begin{align*}\n\\hat\\theta\n&\\triangleq \\argmax_{\\theta}p(\\theta|\\mathcal D) \\\\\n&= \\argmax_{\\theta}p(\\mathcal D|\\theta)p(\\theta) \\\\\n&= \\argmax_{\\theta}[\\log p(\\mathcal D|\\theta)+\\log p(\\theta)]\n\\end{align*}\n$$\n\nNote that under strong sampling assumption, $p(\\mathcal D|\\theta)=\\left[\\frac 1 {\\vert\\theta\\vert}\\right]^I$. Since the likelihood $p(\\mathcal D|\\theta)$ depends exponentially on N, and the prior $p(\\theta)$ stays constant, as we get more and more data, the MAP estimate converges towards the MLE. In other words, if we have enoughd ata, the data overwhelms the prior.\n\n### The Bayesian approach\n\nWhereas MLE and MAP give point estimates of $\\theta$, the Bayesian approach uses the posterior $p(\\theta|\\mathcal D)=\\frac{p(\\mathcal D|\\theta)p(\\theta)}{p(\\mathcal D)}$ to evaluate the predictive distribution i.e. admit many values of the parameters compatible with the data and weigh them according to the posterior density.\n\n$$\np(\\mathbf x|\\mathcal D)=\\int p(\\mathbf x|\\theta)p(\\theta|\\mathcal D)d\\theta\n$$\n\nFrom the Bayesian approach, the posteriors for MAP and MLE can be considered as delta functions centered at their estimates $\\hat\\theta$ i.e. $p(\\theta|\\mathcal D)=\\delta(\\theta-\\hat\\theta)$. Then,\n\n$$\np(\\mathbf x|\\mathcal D)=\\int p(\\mathbf x|\\theta)\\delta(\\theta-\\hat\\theta)d\\theta=p(\\mathbf x|\\hat\\theta)\n$$\n\n## Single r.v. DGM\n\n### Univariate normal distribution\n\n### Univariate categoricacl distribution\n\n## Directed models\n\nLet $G=(U, E)$ be a directed graph where $U$ is the set of nodes and $E$ the set of edges. We associate a random vector $X$ with the graph, where the components of the vector are indexed by the nodes in the graph.\n\nProbability model for a directed graph:\n\n$$\np(x_U|\\theta)=\\prod_{u\\in U}p(x_u|x_{\\pi_u}, \\theta_u)\n$$\n\nConstruct the augmented graphical model for $N$ i.i.d. samples $G^{(N)}=(U^{(N)}, E^{(N)})$. The observed data will be $\\mathcal D=(x_{U, 1}, x_{U, 2}, ..., x_{U, N})$.\n\nProbability model for $G^{(N)}$ and the log-likelihood:\n\n$$\n\\begin{align*}\np(\\mathcal D|\\theta)\n&= \\prod_n p(x_{U, n}|\\theta) \\\\\n&= \\prod_n\\prod_u p(x_{u, n}|x_{\\pi_u, n}, \\theta_u) \\\\\n\\ell(\\theta; \\mathcal D)&=\\sum_n\\sum_u\\log p(x_{u, n}|x_{\\pi_u, n}, \\theta_u)\n\\end{align*}\n$$\n\nNote that the local subset of observations $\\{x_{u, n}, x_{\\pi_u, n}\\}_{n=1}^N$, that is data associated with node $u$ and its parents, is sufficient for $\\theta_u$.\n\n### Directed, discrete models\n\nLet $m(x_U)$ denote the number of times that $x_U$ is observed among the observations in the dataset $\\mathcal D$. Define marginal counts $m(x_C)$ associated with subsets of nodes $C$ as the number of times that configuration $x_C$ is observed in the dataset. A particular subset of interest is the subset consisting of a node $u$ and its parents $\\pi_u$ â€” the family associated with node $u$, denoted as $\\phi_u\\triangleq\\{u\\}\\cup\\pi_u$. We have:\n\n$$\n\\begin{align*}\nm(x_U)&\\triangleq \\sum_n \\delta(x_U-x_{U, n}) \\\\\nm(x_C)&\\triangleq\\sum_{x_U\\setminus C}m(x_U) \\\\\nm(x_{\\phi_u})&\\triangleq\\sum_{x_{U\\setminus\\phi_u}}m(x_U)\n\\end{align*}\n$$\n\nDefine the parameter vector $\\theta_v({x_{\\phi_v}})$ to be a nonnegative, multidimensional table indexed by the joint configuration of $v$ and $\\pi_v$. The normalisation condition requires:\n\n$$\n\\sum_{x_v}\\theta_v(x_{\\phi_v})=\\sum_{x_v}\\theta_v(x_v, x_{\\pi_v})=1\n$$\n\nDefine the local conditional probability of node $v$ using the normalized table:\n\n$$\np(x_v|x_{\\pi(v)}, \\theta_v)\\triangleq\\theta_v(x_{\\phi_v})\n$$\n\nTaking the product over $v$, we obtain the joint probability distribution as the product of normalized potentials:\n\n$$\n\\begin{align*}\np(x_U|\\theta)\n&=\\prod_v p(x_v|x_{\\pi_v}, \\theta_v) \\\\\n&=\\prod_v\\theta_v(x_{\\phi_v})\n\\end{align*}\n$$\n\nWe take a further product over $n$ to obtain the total probability of an i.i.d. dataset $\\mathcal D$:\n\n$$\n\\begin{align*}\np(x_{U, n}|\\theta)&=\\prod_{x_U}p(x_U|\\theta)^{\\delta(x_U-x_{U, n})} \\\\\n\\log p(\\mathcal D|\\theta)\n&= \\log\\prod_n p(x_{U, n}|\\theta) \\\\\n&= \\sum_n\\sum_{x_U}\\delta(x_U-x_{U, n})\\log p(x_U|\\theta)  \\\\\n&= \\sum_{x_U}m(x_U)\\log p(x_U|\\theta) &(1) \\\\\n&= \\sum_{x_U}m(x_U)\\sum_v\\log\\theta_v(x_{\\phi_v}) \\\\\n&= \\sum_v\\sum_{x_{\\phi_v}}\\sum_{x_{U\\setminus\\phi_v}}m(x_U)\\log \\theta_v(x_{\\phi_v}) \\\\\n&= \\sum_v\\sum_{x_{\\phi_v}}\\log \\theta_v(x_{\\phi_v})\\sum_{x_{U\\setminus\\phi_v}}m(x_U) \\\\\n&= \\sum_v\\sum_{\\phi_v}m(x_{\\phi_v})\\log \\theta_v(x_{\\phi_v}) &(2)\n\\end{align*}\n$$\n\n$(1)$ shows that the sum over $n$ has disappeared; we have in essence reduced our representation of joint probability from a function on $G^{(N)}$ to a function on $G$. $(2)$ expresses the log-likelihood as a sum of terms defined on the families ${\\phi_v}$. Furthermore, the likelihood can be seen as a exponential family with $m(x_{\\phi_v})$ as the sufficient statistics and $\\log \\theta_v(x_{\\phi_v})$ as the natural parameters.\n\nTo estimate $\\theta_v(x_{\\phi_v})$, we maximize $m(x_{\\phi_v})\\log \\theta_v(x_{\\phi_v})$ w.r.t. $\\theta_v(x_{\\phi_v})$. Adding a Lagrangian term to handle the normalization constraint, we obtain\n\n$$\n\\hat\\theta_{v, \\text{ML}}(x_{\\phi_v})=\\frac{m(x_{\\phi_v})}{m(x_{\\pi_v})}=\\frac{m(x_v, x_{\\pi_v})}{m(x_{\\pi_v})}\n$$\n\n## Undirected models\n\nUndirected graphical models require an explicit global normalization factor that couples the parameters and complicates the parameter estimation problem. However, for decomposable models, the parameter estimation problem decouples.\n\nParameterize an undirected graphical model via a set of clique potentials $\\psi_C(x_C)$, for $C\\in\\mathcal C$ where $\\mathcal C$ is a set of cliques. Define the joint probability as\n\n$$\np(x_U|\\theta)=\\frac 1 Z\\prod_C \\psi_C(x_C)\n$$\n\nwhere $\\theta=\\{\\psi_C(x_C), C\\in\\mathcal C\\}$ is the collection of parameters, and where $Z$ is the normalization factor $Z=\\sum_{x_U}\\prod_C \\psi_C(x_C)$.\n\nLog-linear form:\n\n$$\np(x_U|\\theta)=\\frac 1 Z\\exp\\left(\\sum_C\\theta_C^T\\phi_C(x_C)\\right)\n$$\n\n### Undirected, discrete models\n\n$$\n\\begin{align*}\np(\\mathcal D|\\theta)&=\\prod_np(x_{U, n}|\\theta) \\\\\n&=\\prod_n\\prod_{x_U}p(x_U|\\theta)^{\\delta(x_U-x_{U, n})} \\\\\n\\ell(\\theta;\\mathcal D)&=\\log p(\\mathcal D|\\theta) \\\\\n&=\\sum_n\\sum_{x_U}\\delta(x_U-x_{U, n})\\log p(x_U|\\theta) \\\\\n&=\\sum_{x_U}m(x_U)\\log p(x_U|\\theta) \\\\\n&=\\sum_{x_U}m(x_U)\\log\\left(\\frac 1 Z\\prod_C \\psi_C(x_C)\\right) \\\\\n&=\\sum_{x_U}m(x_U)\\sum_C\\log\\psi_C(x_C)-\\sum_{x_U}m(x_U)\\log Z \\\\\n&=\\sum_C\\sum_{x_C}m(x_C)\\log\\psi_C(x_C)-N\\log Z\n\\end{align*}\n$$\n\nWe see that the marginal counts $m(x_C)$, for $C\\in\\mathcal C$, are the sufficient statistics for our modlel. This is reminiscent of the directed case, where the cliques $\\mathcal C$ were the families $\\{\\phi_v\\}$.\n\n#### In log-linear form\n\nWe use scaled log-likelihood\n\n$$\n\\ell(\\theta; \\mathcal D)\\triangleq\\frac 1 N\\sum_n\\log p(x_{U, n}|\\theta)=\\frac 1 N\\sum_n\\left[\\sum_C \\theta_C^T\\phi_C(x_C)-\\log Z\\right]\n$$\n\n### MLE\n\nThe derivative of the first term w.r.t $\\psi_C(x_C)$ is $\\frac{m(x_C)}{\\psi_c{x_C}}$. For the second term $\\log Z$,\n\n$$\n\\begin{align*}\n\\frac{\\partial\\log Z}{\\partial\\psi_C(x_C)}\n&=\\frac 1 Z \\frac\\partial{\\partial\\psi_C(x_C)}\\sum_{\\tilde x}\\prod_{\\mathcal D}\\psi_{\\mathcal D}(x_{\\mathcal D}) \\\\\n&=\\frac 1 Z\\sum_{\\tilde x}\\delta(\\tilde x_C-x_C)\\frac\\partial{\\partial\\psi_C(x_C)}\\prod_{\\mathcal D}\\psi_{\\mathcal D}(\\tilde x_{\\mathcal D}) \\\\\n&=\\frac 1 Z\\sum_{\\tilde x}\\delta(\\tilde x_C-x_C)\\prod_{\\mathcal D\\neq C}\\psi_{\\mathcal D}(\\tilde x_{\\mathcal D}) \\\\\n&=\\sum_{\\tilde x}\\delta(\\tilde x_C-x_C)\\frac 1{\\psi_C(\\tilde x_C)}\\frac 1 Z \\prod_{\\mathcal D}\\psi_{\\mathcal D}(\\tilde x_{\\mathcal D}) \\\\\n&=\\frac 1 {\\psi_C(x_C)}\\sum_{\\tilde x}\\delta(\\tilde x_C-x_C)p(\\tilde x) \\\\\n&=\\frac{p(x_C)}{\\psi_C(x_C)}\n\\end{align*}\n$$\n\nTherefore,\n\n$$\n\\frac{\\partial\\ell}{\\partial\\psi_C(x_C)}=\\frac{m(x_C)}{\\psi_c(x_C)}-N\\frac{p(x_C)}{\\psi_C(x_C)}\n$$\n\nAssume WLOG that $\\psi_C(x_C)>0$ and equate the derivatie to zero, we obtain:\n\n$$\n\\hat p_{\\text ML}(x_C)=\\frac 1 N m(x_c)\n$$\n\nDefine the empirical distribution $p_\\text{emp}(x)\\triangleq m(x)/N$ so that $p_\\text{emp}(x_C)\\triangleq m(x_C)/N$ is a marginal under the empirical distribution, we can rewrite the result as:\n\n$$\n\\hat p_{\\text{ML}}(x_C)=p_\\text{emp}(x_C)\n$$\n\nThus we have the following important characterization of MLEs: for each clique $C\\in\\mathcal C$, the model marginals must be equal to the empirical marginals. This forms a system of equations that constrains the MLEs.\n\n- For decomposable models, use the recipe below.\n- For general undirected graphs, use IPF or SGD\n\n#### In log-linear form\n\n$$\n\\frac{\\partial\\ell}{\\partial\\theta_C}=\\mathbb E_{p_{\\text{emp}}}[\\phi_C(x_C)]-\\mathbb E_{p(\\cdot|\\theta)}[\\phi_C(x_C)]\n$$\n\nThis first term is the _clamped term_ and the second _unclamped term_ or _contrastive term_. At the optimum, the gradient is zero, yielding _moment matching_ $\\mathbb E_{p_{\\text{emp}}}[\\phi_C(x_C)]=\\mathbb E_{p(\\cdot|\\theta)}[\\phi_C(x_C)]$.\n\n### Decomposable models\n\nA graph is said to be decomposable if it can be recursively subdivided into disjoint sets $A$, $B$, and $S$ where $S$ separates $A$ and $B$, and where $S$ is complete.\n\nWe can find MLEs for decomposable grphs by inspection, but only if the potentials are defined on maximal cliques i.e. our parameterization must be s.t. the set $\\mathcal C$ ranges over the maximal cliques in the graph. Given this constraint, the recipe is the following:\n\n- for every clique $C$, set the clique potential to the empirical marginal for that clique,\n- for every non-empty intersection between cliques, associate an empirical marginal with that intersection, and divide that empirical marginal into the potential of one of the two cliques that form the intersection.\n\nExample, for the figure below, we would have the following MLE\n\n![A decomposable graph](decomposable-graph.png)\n\n$$\n\\hat p_{\\text{ML}}(x_1, x_2, x_3, x_4)=\\frac{p_\\text{emp}(x_1, x_2, x_3)p_\\text{emp}(x_2, x_3, x_4)}{p_\\text{emp}(x_2, x_3)}\n$$\n\n### Iterative proportional fitting (IPF)\n\nIPF converges and ascends an objective function at each step. It is both a fixed-point algorithm and a coordinate ascent algorithm.\n\n#### IPF as fixed-point iteration\n\nLet us return to the gradient of log-likelihood, but retain the $\\psi_C(x_C)$ factors. We have\n\n$$\n\\frac{p_\\text{emp}(x_C)}{\\psi_C(x_C)}=\\frac{p(x_C)}{\\psi_C(x_C)}\n$$\n\nNote that the parameter $\\psi_c(x_C)$ appears explicitly in this equation in two places, but also appears implicitly in the marginal $p(x_C)$. We can obtain an iterative algo by holding the values of $\\psi_C(x_C)$ fixed on the RHS, and solving for the free parameter $\\psi_C(x_C)$ on the LHS.\n\n$$\\psi_C^{(t+1)}(x_C)=\\psi_C^{(t)}(x_C)\\frac{p_\\text{emp}(x_C)}{p^{(t)}(x_C)}$$\n\n---\n\n**IPF for tabular MRFs**\n\n1. Initialise $\\phi_C=1$ for $C\\in\\mathcal C$.\n2. Repeat until convergence, for $C\\in\\mathcal C$:\n   - $p_C=p(x_C|\\phi)$.\n   - $\\hat p_C=p_{\\text{emp}}(x)$.\n   - $\\phi_C=\\phi_C\\times\\frac{\\hat p_C}{p_C}$\n\n---\n\nProperties of the IPF update equation:\n\n- the marginal $p^{(t+1)}(x_C)$ is equal to the empirical marginal $p_\\text{emp}(x_C)$, and\n- the normalization factor $Z$ remains constant across IPF updates.\n\nProof:\n\n$$\n\\begin{align*}\np^{(t+1)}(x_C)\n&=\\sum_{x_{U\\setminus C}}p^{(t+1)}(x) \\\\\n&=\\sum_{x_{U\\setminus C}}\\frac 1 {Z^{(t+1)}}\\prod_{\\mathcal D}\\psi_{\\mathcal D}^{(t+1)}(x_{\\mathcal D}) \\\\\n&=\\frac 1 {Z^{(t+1)}}\\sum_{x_{U\\setminus C}}\\psi_C^{(t+1)}(x_C)\\prod_{\\mathcal D\\neq C}\\psi_{\\mathcal D}^{(t)}(x_{\\mathcal D}) \\\\\n&=\\frac 1 {Z^{(t+1)}}\\sum_{x_{U\\setminus C}}\\psi_C^{(t)}(x_C)\\frac{p_\\text{emp}(x_C)}{p^{(t)}(x_C)}\\prod_{\\mathcal D\\neq C}\\psi_{\\mathcal D}^{(t)}(x_{\\mathcal D}) \\\\\n&=\\frac{Z^{(t)}}{Z^{(t+1)}}\\frac{p_\\text{emp}(x_C)}{p^{(t)}(x_C)}\\sum_{x_{U\\setminus C}}\\frac 1{Z^{(t)}}\\prod_{\\mathcal D}\\psi_{\\mathcal D}^{(t)}(x_{\\mathcal D}) \\\\\n&=\\frac{Z^{(t)}}{Z^{(t+1)}}\\frac{p_\\text{emp}(x_C)}{p^{(t)}(x_C)}p^{(t)}(x_C) \\\\\n&=\\frac{Z^{(t)}}{Z^{(t+1)}}p_\\text{emp}(x_C)\n\\end{align*}\n$$\n\nNote that both $p^{(t+1)}(x_C)$ and $p_\\text{emp}(x_C)$ are normalized. Thus, summing both sides of the above equations w.r.t $x_C$, we get $Z^{(t+1)}=Z^{(t)}$. This further implies that $p^{(t+1)}(x_C)=p_\\text{emp}(x_C)$.\n\nIPF in terms of joint probabilities:\n\n$$\n\\begin{align*}\np^{(t+1)}(x_U)\n&=p^{(t)}(x_U)\\frac{p_\\text{emp}(x_C)}{p^{(t)}(x_C)} \\\\\n&=p^{(t)}(x_{U\\setminus C}|x_C)p_\\text{emp}(x_C)\n\\end{align*}\n$$\n\nInterpretation: IPF iteration rettains the \"old\" conditional probability $p^{(t)}(x_{U\\setminus C}|x_C)$ while replacing the \"old\" marginal probability $p^{(t)}(x_C)$ with the new marginal $p_\\text{emp}(x_C)$.\n\nIn the case of decomposable models, IPF converges in one iteration.\n\n#### IPF as coordinate ascent\n\nA \"coordinate\" in this setting is a potential function. Take the derivative of log-likelihood w.r.t the coordiante $\\psi_C(x_C)$, fior fixed $C$ and varying $x_C$, and solve for the maximizing values of these parameters while holding the remaining potentials fixed.\n\nFrom the earlier derivation,\n\n$$\n\\frac{\\partial\\ell}{\\partial \\psi_C(x_C)}=\n\\frac{m(x_C)}{\\psi_c{x_C}}-\\frac N Z\\sum_{\\tilde x}\\delta(\\tilde x_C-x_C)\\prod_{\\mathcal D\\neq C}\\psi_{\\mathcal D}(\\tilde x_{\\mathcal D})\n$$\n\nTake parameter $\\psi_C(x_C)$ on the RHS as a variable whose maximizing value we wish to solve for, where the remaining parameters $\\psi_{\\mathcal D}(x_{\\mathcal D})$ are held fixed. Since $Z$ remains constant during IPF updates,\n\n$$\n\\begin{align*}\n\\frac{\\partial\\ell}{\\partial \\psi_C(x_C)}\n&=\\frac{m(x_C)}{\\psi_c^{(t+1)}{x_C}}-\\frac N Z\\sum_{\\tilde x}\\delta(\\tilde x_C-x_C)\\prod_{\\mathcal D\\neq C}\\psi_{\\mathcal D}^{(t)}(\\tilde x_{\\mathcal D}) \\\\\n&=\\frac{m(x_C)}{\\psi_c^{(t+1)}{x_C}}-\\frac N{\\psi_C^{(t)}(x_C)}\\sum_{\\tilde x}\\delta(\\tilde x_C-x_C)\\frac 1 Z\\prod_{\\mathcal D}\\psi_{\\mathcal D}^{(t)}(\\tilde x_{\\mathcal D}) \\\\\n&=\\frac{m(x_C)}{\\psi_c^{(t+1)}{x_C}}-\\frac N{\\psi_C^{(t)}(x_C)}p^{(t)}(x_C)\n\\end{align*}\n$$\n\nand we see that the IPF update equation $\\psi_C^{(t+1)}(x_C)=\\psi_C^{(t)}\\frac{p_\\text{emp}(x_C)}{p^{(t)}(x_C)}$ does indeed set the gradient of the log-likelihood to zero, and thus a coordinate ascent step.\n\n### Gradient descent\n\n$$\n\\psi_C^{(t+1)}(x_C)=\\psi_C^{(t)}(x_C)+\\frac\\rho{\\psi_C^{(t)}(x_C)}\\left(p_\\text{emp}(x_C)-p^{(t)}(x_C)\\right)\n$$\n\nwhere $\\rho$ is a step size. We see that the difference between the empirical marginals and the model marginals drives the algorithm.\n\nAdvantage: All parameters can be adjusted silmultaneously (although a variant of IPF can also achieve this).\n\nDisadvantages:\n\n- The need to choose a step size.\n- The noramlization factor $Z$ does not remain constant and must be recalculated anew after each iteration.\n\n### In log-linear form\n\n$$\n\\triangledown_{\\mathbb\\theta}\\ell(\\mathbb\\theta; \\mathcal D)=\\frac 1 N\\sum_n[\\bm\\phi(x_{U,n})-\\mathbb E[\\bm\\phi(x_U)]]\n$$\n\nWe can approximate the model expectations using MC sampling.\n\n---\n\n**SGD ML for fitting an MRF**\n\n1. initialise weights $\\bm\\theta$ randomly.\n2. $k=0, \\eta=1$.\n3. for each epoch, for each minibatch of size $B$:\n   - for each sample $s=1:S$, sample $x^{s, k}\\sim p(x|\\theta_k)$.\n   - $\\hat{\\mathbb E}[\\bm\\phi(x)]=\\frac 1 S\\sum_s\\bm\\phi(x^{s, k})$.\n   - for each training case $i$ in minibatch, $\\mathbf g_{ik}=\\bm\\phi(x_i)-\\hat{\\mathbb E}[\\bm\\phi(x)]$.\n   - $\\mathbf g_k=\\frac 1 B\\sum_{i\\in B}\\mathbf g_{ik}$.\n   - $\\bm\\theta_{k+1}=\\bm\\theta_k-\\eta\\mathbf g_{k}$.\n   - $k = k+1$.\n   - Decrease step size $\\eta$.\n\n---\n\n## CRFs\n\n## Reference materials\n\n- Prince, S. J. D. (2012). Chapter 4: Fitting probability models. In _Computer Vision: Models, Learning, and Inference_ (pp. 28-43). Cambridge University Press.\n- Murphy, K. P. (2012). Undirected graphcial models (Markov random fields). In _Machine Learning: A Probabilistic Perspective_ (pp. 676-684). The MIT Press.\n- Jordan, M. I. (2002). Chapter 9: Completely Observed Graphical Models. In _An Introduction to Probabilistic Graphical Models_.\n"}},"__N_SSG":true}