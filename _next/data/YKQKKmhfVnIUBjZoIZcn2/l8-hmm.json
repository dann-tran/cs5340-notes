{"pageProps":{"metadataList":[{"slug":"l2-bayesian-networks","title":"Bayesian Networks (Directed Graphical Models)","tag":"representation","lectureNumber":2},{"slug":"l3-mrf","title":"Markov Random Fields (Undirected Graphical Models)","tag":"representation","lectureNumber":3},{"slug":"l4-variable-elimination-and-belief-propagation","title":"Variable Elimination and Belief Propogation","tag":"exact inference","lectureNumber":4},{"slug":"l5-factor-graph-and-jt-algo","title":"Factor Graph and Junction Tree Algorithm","tag":"exact inference","lectureNumber":5},{"slug":"l6-parameter-learning-with-complete-data","title":"Parameter Learning with Complete Data","tag":"learning","lectureNumber":6},{"slug":"l7-mixture-models-and-em-algo","title":"Mixture Models and the EM Algorithm","tag":"learning","lectureNumber":7},{"slug":"l8-hmm","title":"Hidden Markov Models (HMMs)","tag":"modelling","lectureNumber":8},{"slug":"l9-monte-carlo-inference","title":"Monte Carlo Inference (Sampling)","tag":"approximate inference","lectureNumber":9},{"slug":"l10-variational-inference","title":"Variational Inference","tag":"approximate inference","lectureNumber":10},{"slug":"l11-variational-autoencoder-and-mixture-density-networks","title":"Variational Autoencoder and Mixture Density Networks","tag":"modelling","lectureNumber":11},{"slug":"l12-graph-cut-and-alpha-expansion","title":"Graph-Cut and Alpha-Expansion","tag":"modelling","lectureNumber":12}],"post":{"metadata":{"slug":"l8-hmm","title":"Hidden Markov Models (HMMs)","tag":"modelling","lectureNumber":8},"markdownBody":"\n## The graphical model\n\nSequential data: datasets in which successive samples are no longer assumed to be independent.\n\n### Markov models\n\nFor discrete time steps:\n\n$$\np(x)=p(x_1)\\prod_{t=2}^Tp(x_t|x_{t-1})\n$$\n\nIf the transition probability $p(x_t|x_{t-1})$ is independent of time, then the chain is called homogenous, stationary, or time-invariant.\n\nIf $x_t$ is discrete, we can represent $p(x_t|x_{t-1})$ as a transition matrix $A$, where $A_{ij}=p(X_t=j|X_{t-1}=i)$. Each row of the matrix sums to one, $\\sum_j A_{ij}=1$, so this is called a stochastic matrix.\n\n### HMM\n\nAn HMM is a natural generalization of a mixture model, viewed as a \"dynamical\" mixture model, where we no longer assume that the states (i.e. mixture components) are chosen independently at each step, but that the choice of a state at a given step depends on the choice of the state at the previous step. Thus we augment the basic mixture model to include a matrix of transition probabilites linking the states at neighbouring steps.\n\nA HMM consists of a discrete-time, discrete-state Markov chain, plus an observation model aka emission probability $p(\\mathbf x_t|z_t)$. The joint distribution:\n\n$$\np(\\mathbf z_{1:T}, \\mathbf x_{1:T}|\\bm\\theta)\n=p(\\mathbf z_{1:T}|\\bm\\theta)p(\\mathbf x_{1:T}|z, \\bm\\theta)\n=p(z_1|\\bm\\pi)\\prod_{t=2}^Tp(z_t|z_{t-1}, A)\\prod_{t=1}^Tp(\\mathbf x_t|z_t, \\bm\\phi)\n$$\n\nwhere\n\n- Parameters $\\bm\\theta=\\{\\bm\\pi, A, \\bm\\phi\\}$\n- Initial state distribution $p(z_1=i)=\\pi_i$\n- Transition matrix $A_{i, j}=p(z_t=j|z_{t-1}=i)$\n- Class-conditional or emission densities $p(\\mathbf x_t|z_t=k, \\bm\\phi)$. E.g.\n  - Discrete: $p(\\mathbf x_t=l|z_t=k, \\bm\\phi)=B_{k, l}$ where $B$is an observation matrix.\n  - Continuous: $p(\\mathbf x_t|z_t=k, \\bm\\phi)=\\mathcal N(\\mathbf x_t|\\bm\\mu_k, \\bm\\Sigma_k)$\n\n## Inference in HMMs\n\n### Types of inference problems for temporal models\n\nFiltering: computes the belief state $p(z_t|\\mathbf x_{1:t})$ online as the data streams in.\n\nSmoothing: computes $p(z_t|\\mathbf x_{1:t})$ offline, given all the evidence.\n\nFixed-lag smoothing: computes $p(z_{t-\\ell}|\\mathbf x_{1:t})$ where $\\ell>0$ is the lag. This is a compromise between online and offline estimation and gives better performance than filtering, but incurs a slight delay. By changing the size of the lag, one can trade off accuracy vs delay.\n\nPrediction: computes $p(z_{t+h}|\\mathbf x_{1:t})$ where $h>0$ is the prediction horizon.\n\nMAP estimation: computes $\\argmax_{\\mathbf z_{1:T}}p(\\mathbf z_{1:T}|\\mathbf x_{1:T})$. This is known as Viterbi decoding in the context of HMMs.\n\nPosterior samples: $\\mathbf z_{1:T}\\sim p(\\mathbf z_{1:T}|\\mathbf x_{1:T})$. This can be done when there is more than one plausiable interpretation of the data.\n\nProbability of the eivdence: $p(\\mathbf x_{1:T})=\\sum_{\\mathbf z_{1:T}}p(\\mathbf z_{1:T}, \\mathbf x_{1:T})$\n\n![The main kinds of inference for state-space models](inference-state-space-models.png)\n\n### The forwards algo (online)\n\nGoal: compute filtered marginal $\\bm\\alpha_t=p(z_t|\\mathbf x_{1:t})$ aka _filtered belief state_ at time $t$.\n\nPrediction step: computes the one-step-ahead predictive density; this acts as the new prior for time $t$.\n\n$$\np(z_t=j|\\mathbf x_{1:t-1})=\\sum_i p(z_t=j|z_{t-1}=i)p(z_{t-1}=i|\\mathbf x_{1:t-1})\n$$\n\nUpdate step: absorved the observed data from time $t$ using Bayes rule\n\n$$\n\\begin{align*}\n\\alpha_t(j)\n&\\triangleq p(z_t=j|\\mathbf x_{1:t}) \\\\\n&=\\frac 1{Z_t}p(\\mathbf x_t|z_t=j)p(z_t=j|\\mathbf x_{1:t-1}) \\\\\n&=\\frac 1{Z_t}\\psi_t(j)\\sum_i\\alpha_{t-1}(j)\\psi(i, j)\n\\end{align*}\n$$\n\nwhere $Z_t\\triangleq p(\\mathbf x_t|\\mathbf x_{1:t-1})=\\sum_j \\psi_t(j)\\sum_i\\alpha_{t-1}(j)\\psi(i, j)$\n\nIn matrix vector notation:\n\n$$\n\\bm\\alpha_t\\propto\\bm\\psi_t\\odot(\\bm\\Psi^T\\bm\\alpha_{t-1})\n$$\n\nwhere $\\psi_t(j)=p(\\mathbf x_t|z_t=j)$ is the local evidence at time $t$, $\\Psi(i, j)=p(p_t=j|z_{t-1}=i)$ is the transition matrix.\n\nBase case:\n\n$$\n\\alpha_1(j)=p(z_1=j|\\mathbf x_1)=\\psi_1(j)\n$$\n\n### The forward-backward algo (offline)\n\nGoal: compute the smoothed posterior marginal $\\gamma_t(j)\\triangleq p(z_t=j|\\mathbf x_{1:T})$.\n\n$$\n\\begin{align*}\n\\gamma_t(j)\n&=p(z_t=j|\\mathbf x_{1:T}) \\\\\n&\\propto p(z_t=j, \\mathbf x_{t+1:T}|\\mathbf x_{1:t}) \\\\\n&\\propto p(z_t=j|\\mathbf x_{1:t})p(\\mathbf x_{t+1:T}|z_t=j) \\\\\n&=\\alpha_t(j)\\beta_t(j)\n\\end{align*}\n$$\n\nwhere $\\beta_j(j)\\triangleq p(\\mathbf x_{t+1:T}|z_t=j)$ is the conditional likelhiood of future evidence given the hidden state at time $t$ is $j$.\n\nThe forwards algo recursively computes the $\\alpha$'s in a left-to-right fashion. We'll recursively compute the $\\beta$'s in a right-to-left fashion.\n\n$$\n\\begin{align*}\n\\beta_{t-1}(i)\n&=p(\\mathbf x_{t:T}|z_{t-1}=i) \\\\\n&=\\sum_j p(z_t=j, \\mathbf x_{t:T}|z_{t-1}=i) \\\\\n&=\\sum_j p(\\mathbf x_{t+1:T}|z_t=j)p(\\mathbf x_t|z_t=j)p(z_t=j|z_{t-1}=i) \\\\\n&=\\sum_j \\beta_t(j)\\psi_t(j)\\psi(i, j)\n\\end{align*}\n$$\n\nWe can write the resulting equation in matrix-vector form as\n\n$$\n\\bm\\beta_{t-1}=\\bm\\Psi(\\bm\\psi_t\\odot\\bm\\beta_t)\n$$\n\nThe base case is\n\n$$\n\\beta_T(i)=p(\\mathbf x_{T+1:T}|z_T=i)=p(\\empty|z_T=i)=1\n$$\n\n### Two-slice smoothed marginals\n\nWhen we use EM for learning, we'll need to compute the expected number of transitions from state $i$ to state $j$:\n\n$$\n\\begin{align*}\nN_{ij}\n&=\\sum_{t=1}^{T-1}\\mathbb E[\\mathbb I(z_t=i, z_{t+1}=j)|\\mathbb x_{1:T}] \\\\\n&=\\sum_{t=1}^Tp(z_t=i, z_{t+1}=j|\\mathbb x_{1:T})\n\\end{align*}\n$$\n\nDefine the (smoothed) two-slice marginal\n\n$$\n\\begin{align*}\n\\xi_{t, t+1}(i, j)\n&\\triangleq p(z_t=i, z_{t+1}=j|\\mathbb x_{1:T}) \\\\\n&\\propto p(z_t|\\mathbf x_{1:t})p(z_{t+1}|z_t, \\mathbf x_{t+1:T}) \\\\\n&\\propto p(z_t|\\mathbf x_{1:t})p(\\mathbf x_{t+1:T}|z_t, z_{t+1})p(z_{t+1}|z_t) \\\\\n&\\propto p(z_t|\\mathbf x_{1:t})p(\\mathbf x_{t+1}|z_{t+1})p(\\mathbf x_{t+2:T}|z_{t+1})p(z_{t+1}|z_t) \\\\\n&=\\alpha_t(i)\\psi_{t+1}(j)\\beta_{t+1}(j)\\psi(i, j)\n\\end{align*}\n$$\n\nIn matrix-vector form:\n\n$$\n\\bm\\xi_{t, t+1}\\propto\\bm\\Psi\\odot(\\bm\\alpha_t(\\bm\\psi_{t+1}\\odot\\bm\\beta_{t+1})^T)\n$$\n\n### Time and space complexity\n\nA straightforward implementation of FB takes $O(K^2T)$ time since we must perform a $K\\times K$ matrix multiplication at each step. If the transition matrix is sparse, we can reduce this substantially e.g. $O(TK)$ for a left-to-right transition matrix.\n\nThe expected sufficient statistics needed by EM are $\\sum_t\\xi_{t-1, t}(i, j)$ which takes constant space. However, to compute them, we need $O(KT)$ working space, since we must store $\\{\\alpha_t\\}_{t=1}^T$ until we do the backwards pass. It is possible to devise a simple divide-and-conquer algo that reduces the space complexity from $O(KT)$ to $O(K\\log T)$ at the cost of increasing the running time from $O(K^2T)$ to $o(K^2T\\log T)$.\n\n## The Viterbi algo\n\nThe Viterbi algo computes the most probable sequence of states in a chain-structured grpahical model i.e. MAP\n\n$$\n\\mathbf z^*=\\argmax_{\\mathbf z_{1:T}}p(\\mathbf z_{1:t}|\\mathbf x_{1:T})\n$$\n\nThis is equivalent to computing a shortest path through the trellis diagram where the nodes are possible states at each time step, and the node and edge weights are log-probabilities i.e. the weight of a path $(z_t)_{t=1}^T$ is given by\n\n$$\n\\log\\pi_1(z_1)+\\log\\psi_1(z_1)+\\sum_{t=2}^T[\\log\\psi(z_{t-1}, z_t)+\\log\\psi_t(z_t)]\n$$\n\nWe cannot simply replace the sum-operator in forwards-backwards (sum-product) with a max-operator. In general max-product can lead to incorrect results if there are multiple equally probably joint assignments. This is because each node breaks ties independently and hence may do so in a manner that is inconssitent with its neighbours.\n\nThe Viterbi algo uses max-product for the forward pass and a traceback procedure for backward pass to recover the most probable path through the trellis of states. Once $z_t$ pics its most probable state, the previous nodes condition on this event, and therefore they will break ties consistently.\n\nDefine the probability of ending up in state $j$ at time $t$, given that we take the most probable path.\n\n$$\n\\delta_t(j)\\triangleq\\max_{z_1, ..., z_{t-1}} p(\\mathbf z_{1:t-1}, z_t=j|\\mathbf x_{1:t})\n$$\n\nThe most probable path to state $j$ at time $t$ must consist of the most probable path to some other state $i$ at time $t-1$, followed by a transition from $i$ to $j$. Hence\n\n$$\n\\delta_t(j)=\\max_i \\delta_{t-1}(i)\\psi(i, j)\\psi_t(j)\n$$\n\nWe also keep track of the most likely previous state on the most probable path to $z_t=j$ for all $j$:\n\n$$\na_t(j)=\\argmax_i\\delta_{t-1}(i)\\psi(i, j)\\psi_t(j)\n$$\n\nWe initialize by setting $\\delta_1(j)=\\pi_j\\psi_1(j)$ and terminate by computing the most probable final state $z_T^*$:\n\n$$\nz_T^*=\\argmax_i\\delta_T(i)\n$$\n\nWe can then compute the most probable sequence of states using traceback:\n\n$$\nz_t^*=a_{t+1}(z_{t+1}^*)\n$$\n\nNumerical underflow: work in log domain. We can use\n\n$$\n\\begin{align*}\n\\log\\delta_t(j)\n&\\triangleq \\max_{\\mathbf z_{1:t-1}}\\log p(\\mathbf z_{1:t-1}, z_t=j|\\mathbf x_{1:t}) \\\\\n&=\\max_i\\log\\delta_{t-1}(i)+\\log\\psi(i, j)+\\log\\psi_t(j)\n\\end{align*}\n$$\n\n### The sum-product algo for HMM\n\n## Learning: EM for HMMs (the Baum-Welch algo)\n\nConsider $N$ i.i.d. replicates.\n\n### E-step\n\n$$\n\\mathcal Q(\\bm\\theta, \\bm\\theta^{\\text{old}})=\\sum_{k=1}^K\\mathbb E[N_k^1]\\log\\pi_k+\\sum_{j=1}^K\\sum_{k=1}^K\\mathbb E[N_{jk}]\\log A_{jk}+\\sum_{k=1}^KE[N_j]\\log p(\\mathbf x_{i, t}|\\bm\\phi_k)\n$$\n\nwhere the expected counts, computed using $\\bm\\theta^{\\text{old}}$, are given by\n\n$$\n\\begin{align*}\n\\mathbb E[N_k^1]&=\\sum_{i=1}^N\\gamma_{i, 1}(j) \\\\\n\\mathbb E[N_{jk}]&=\\sum_{i=1}^N\\sum_{t=2}^{T_i}\\xi_{i, t}(j, k) \\\\\n\\mathbb E[N_j]&=\\sum_{i=1}^N\\sum_{t=1}^{T_i}\\gamma_{i, t}(j)\n\\end{align*}\n$$\n\n### M-step\n\n$$\n\\hat A_{jk}=\\frac{\\mathbb E[N_{jk}]}{\\sum_{k'}\\mathbb E[N_{jk'}]}, \\space\n\\hat\\pi_k=\\frac{\\mathbb E[N_k^1]}{N}\n$$\n\n#### Multinoulli observation model\n\nThe expected sufficient statistics are\n\n$$\n\\begin{align*}\n\\mathbb E[M_{jl}]\n&=\\sum_{i=1}^N\\sum_{t=1}^{T_i}\\gamma_{i, t}(j)\\mathbb I(x_{i, t}=l) \\\\\n&=\\sum_{i=1}^N\\sum_{t:x_{i, t}=l}\\gamma_{i, t}(j)\n\\end{align*}\n$$\n\nThe M-step has the form\n\n$$\n\\hat B_{jl}=\\frac{\\mathbf E[M_{jl}]}{\\mathbf E[N_j]}\n$$\n\n#### Gaussian observation model\n\nThe expected sufficient statistics are\n\n$$\n\\begin{align*}\n\\mathbb E[\\overline{\\mathbf x}_k] &= \\sum_{i=1}^N\\sum_{t=1}^{T_i}\\gamma_{i, t}(k)\\mathbf x_{i, t} \\\\\n\\mathbb E[(\\overline{\\mathbf x\\mathbf x})_k^T] &= \\sum_{i=1}^N\\sum_{t=1}^{T_i}\\gamma_{i, t}(k)\\mathbf x_{i, t}\\mathbf x_{i, t}^T \\\\\n\\end{align*}\n$$\n\nThe M-step becomes\n\n$$\n\\hat{\\bm\\mu}_k=\\frac{\\mathbb E[\\overline{\\mathbf x}_k]}{\\mathbb E[N_k]}, \\space\n\\hat{\\bm\\Sigma}_k=\\frac\n{\\mathbb E[(\\overline{\\mathbf x\\mathbf x})_k^T]-\\mathbb E[N_k]\\hat{\\bm\\mu}_k\\hat{\\bm\\mu}_k^T}\n{\\mathbb E[N_k]}\n$$\n\n## Reference materials\n\n- Murphy, K. P. (2012). Chapter 17: Markov and Hidden Markov Models. In _Machine Learning: A Probabilistic Perspective_. The MIT Press.\n"}},"__N_SSG":true}