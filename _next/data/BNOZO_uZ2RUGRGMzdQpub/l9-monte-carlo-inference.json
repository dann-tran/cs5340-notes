{"pageProps":{"metadataList":[{"slug":"l9-monte-carlo-inference","title":"Monte Carlo Inference (Sampling)"}],"post":{"metadata":{"slug":"l9-monte-carlo-inference","title":"Monte Carlo Inference (Sampling)"},"markdownBody":"\nSampling methods can be used to perform intractable integrations c e.g. normalization, marginalization, expectation.\n\n## The Monte Carlo principle\n\nMC simulation draws an i.i.d. set of samples $\\{x^{(i)}\\}_{i=1}^N$ from a target density $p(x)$ defined on a high-dimenstional space $\\mathcal{X}$. These $N$ samples can be used to approximate the target density with the empirical point-mass function\n\n$$p(x)\\approx\\frac{1}{N}\\sum_{i=1}^N\\mathcal{1}_{x^{(i)}}(x),$$\n\nwhere $\\mathcal{1}_{x^{(i)}}(x)$ denotes indicator function at $x^{(i)}$.\n\nConsequently, to approximate expectation $\\mathbb{E}_{x\\sim p}[f(x)]$ with tractable sums $I_N$ that converge,\n\n$$\\frac{1}{N}\\sum_{i=1}^Nf(x^{(i)})=I_N\\overset{\\text{a.s.}}{\\underset{N\\rightarrow\\infty}{\\longrightarrow}}\\mathbb{E}_{x\\sim p}[f(x)]=\\int_\\mathcal{X}f(x)p(x)dx$$\n\n- The estimate $I_N$ is unbiased, and by SLLN, it will almost surely (a.s.) to $I(f)$.\n- If the variance of $\\text{Var}_{x\\sim p}[f(x)]<\\infty$, then $\\text{Var}_{x^i\\overset{\\text{i.i.d.}}{\\sim}p}[I_N]=\\frac{1}{N}\\text{Var}_{x\\sim p}[f(x)]$.\n- CLT yields convergence in distribution of the error $\\sqrt{N}(I_N-\\mathbb{E}_{x\\sim p}[f(x)])\\underset{N\\rightarrow\\infty}{\\implies}\\mathcal{N}(0, \\sigma_f^2)$.\n\n## Standard distributions\n\nTo sample from $p(x)$ with a closed-form inverse CDF:\n\n1. Sample $u\\sim\\mathcal{U}(0, 1)$.\n2. $x=F_X^{-1}(u)$.\n\nIn the case of a multinomial distribution with $k$ possible outcomes and associated probabilities $\\theta_1, ..., \\theta_k$.\n\n- Subdivide a unit interval into $k$ regions with region $i$ having size $\\theta_i$.\n- Sample uniformly from $[0, 1]$ and return the value of the region in which our sample falls.\n\n![Reducing sampling from a multinomial distribution to sampling a uniform distribution in $[0, 1]$](multinomial-sampling.png)\n\n## Forward Sampling (aka Ancestral Sampling)\n\n### Sample from a prior\n\nTo sample from prior $p(x)=\\prod_{i=1}^M p(x_i|{\\bf \\pi}_i)$ of a DGM, simply sample nodes in topological order, conditioned on the sampled values of the parent nodes.\n\n\"Forward sampling\" can also be performed efficiently on undirected models if the model can be represented by a clique tree with a small number of variables per node (clique).\n\n- Calibrate the clique tree, which gives us the marginal distribution over each node, and choose a node to be the root.\n- To sample for each node:\n  1. Marginalize over variables to get the marginal for a single variable $p(X_1|E=e)$.\n  2. Sample $x_1\\sim p(X_1|E=e)$ and incorporate $X_1=x_1$ as evidence.\n  3. Proceed with sampling $x_2\\sim p(X_2=x_2|X_1=x_1, E=e)$, $x_3\\sim p(X_3=x_3|X_1=x_1, X_2=x_2, E=e)$ and so on.\n- When moving down the tree to sample variables from other nodes, each node must send an updated message containing the values of the sampled variables.\n\n### Sample from a posterior\n\nSuppose $X=Z\\cup E$. To sample from posterior $p(z|e)$, use forwards sampling on prior $p(x)$ and throw away all samples that are inconsistent with $e$ (i.e. logic sampling, which can be considered a special case of rejections sampling). Formally,\n\n$$p(e)=\\sum_zp(e,z)dz=\\sum_xp(x)\\mathbb{1}_e(x)dx=\\mathbb{E}_{x\\sim p}[\\mathbb{1}_e(x)]\\approx\\frac{1}{N}\\sum_{i=1}^N\\mathbb{1}_e(x^{(i)})$$\n\n$$p(z|e)=\\frac{p(e,z)}{p(e)}\\approx\\frac{\\frac{1}{N}\\sum_{i=1}^{N}\\mathbb{1}_{e,z}(x^{(i)})}{\\frac{1}{N}\\sum_{i=1}^N\\mathbb{1}_{e}(x^{(i)})}=\\frac{\\sum \\mathbb{1}_{e,z}(x^{(i)})}{\\sum \\mathbb{1}_{e}(x^{(i)})}$$\n\nwhere $\\mathbb{1}_e(x)=\\begin{cases}1&\\text{if }x\\text{ is consistent with }e \\\\0&\\text{otherwise}\\end{cases}$.\n\nDrawback: the overall probability of accepting a sample from the posterior decreases rapidly as the number of observed variables increases and as the number of states that those variables can take increases.\n\n## Rejection sampling\n\nSuppose that:\n\n- Target distribution is $p(x)$ hard to sample directly from.\n- $p(x)$ can be evaluated up to a normalising constant i.e. unormalized potential $\\tilde{p}(x)$ can be readily evaluated, but $p(x)=\\frac{1}{Z_p}\\tilde{p}(x)$ is not ($Z_p$ is unknown).\n- Proposal distribution $q(x)$ is easy to sample from.\n\nChoose $M<\\infty$ such that $\\tilde{p}(x)\\leq Mq(x)$.\n\n---\n\n**Rejection Sampling Algorithm**\n\nSet $i=1$.\n\nRepeat until $i=N$:\n\n1. Sample $x^{(i)}\\sim q(x)$ and $u\\sim\\mathcal{U}(0, 1)$.\n2. If $u<\\frac{\\tilde{p}(x^{(i)})}{Mq(x^{(i)})}$ then accept $x^{(i)}$ and increment the counter $i$ by $1$. Otherwise, reject.\n\n---\n\n![Rejection sampling](rejection-sampling.png)\n\nLimitations:\n\n- It is not always possible to bound $p(x)/q(x)$ within a reasonable constant $M$ over the whole sapce $\\mathcal{X}$.\n- If $M$ is too large, the acceptance probability $\\text{Pr}(x\\text{ accepted})=\\text{Pr}\\left(u<\\frac{\\tilde{p}(x)}{Mq(x)}\\right)\\approx\\frac{1}{M}$ will be too small.\n\n## Importance sampling\n\nImportance sampling takes all samples drawn from $q$ and reweigh them with _importance weights_. We must assume that the support of $q(x)$ includes the support of $p(x)$.\n\n### Unnormalised importance sampling\n\nWhen $p(x)$ can be evaluated, we can define $w(x)\\triangleq p(x)/q(x)$. We thus have\n\n$$\n\\begin{align*}\n\\mathbb{E}_{x \\sim p}[f(x)]\n&= \\int f(x)p(x)dx \\\\\n&= \\int f(x) \\frac{p(x)}{q(x)} q(x) dx \\\\\n&= \\mathbb{E}_{x\\sim q}[f(x)w(x)]\n\\end{align*}\n$$\n\nThe Monte Carlo estimate is\n\n$$\\hat{\\mathbb{E}}_{x\\sim p}[f(x)]=\\frac{1}{N} \\sum_{i=1}^N f(x^{(i)}) w(x^{(i)})$$\n\nThe variance of this new estimator is $\\text{Var}[\\hat{\\mathbb{E}}_{x\\sim p}[f(x)]]=\\frac{1}{N}\\text{Var}_{x \\sim q}[f(x)w(x)]$ where\n\n$$\n\\begin{align*}\n\\text{Var}_{x \\sim q}[f(x)w(x)]\n&= \\mathbb{E}_{x\\sim q}\\left[\\left(f(x)w(x)\\right)^2\\right] - \\mathbb{E}_{x\\sim q}[f(x)w(x)]^2 \\\\\n&= \\mathbb{E}_{x\\sim q}\\left[\\left(f(x)w(x)\\right)^2\\right] - \\mathbb{E}_{x\\sim p}[f(x)]^2 \\\\\n&\\geq 0\n\\end{align*}\n$$\n\nTo minimize the variance, we only need to minimize $\\mathbb{E}_{x\\sim q}\\left[\\left(f(x)w(x)\\right)^2\\right]$ because $\\mathbb{E}_{x\\sim p}[f(x)]^2$ does not depend on $q$. According to Jensen's inequality,\n\n$$\n\\begin{align*}\n\\mathbb{E}_{x\\sim q}\\left[\\left(f(x)w(x)\\right)^2\\right]\n&\\geq \\mathbb{E}_{x\\sim q}[\\vert f(x)\\vert w(x)]^2 \\\\\n&= \\left(\\int \\vert f(x)\\vert p(x)dx\\right)^2 \\\\\n&= \\mathbb{E}_{x\\sim p}[\\vert p(x)\\vert]^2 \\\\\n\\end{align*}\n$$\n\nThis lower bound can be attained by choosing _optimal importance distribution_\n\n$$q^*(x)=\\frac{\\vert f(x)\\vert p(x)}{\\int{\\vert f(x)\\vert p(x)}}$$\n\nIf we can sample from this $q$ (and evaluate the corresponding weight), then we only need a single MC sample to compute the true value of our integral. However, sampling from such a $\\tilde{q}(x)=\\vert f(x)\\vert p(x)$ is NP-hard in general. Nevertheless, this tells us that high sampling efficiency is achieved when we focus on sampling from $p(x)$ in the important regions where $\\vert f(x)\\vert p(x)$ is relatively large.\n\n### Normalised importance sampling\n\nAssume only $\\tilde{p}(x)$ can be evaluated, we then define $w(x)\\triangleq\\tilde{p}(x)/q(x)$. We have\n\n$$X_p=\\int \\tilde{p}(x)dx=\\int \\frac{\\tilde{p}(x)}{q(x)}q(x)dx=\\mathbb{E}_{x\\sim q}[w(x)]$$\n\nAs a result,\n\n$$\n\\begin{align*}\n\\mathbb{E}_{x\\sim p}[f(x)]\n&= \\int f(x)p(x)dx \\\\\n&= \\frac{1}{X_p}\\int f(x)\\frac{\\tilde{p}(x)}{q(x)}q(x)dx \\\\\n&= \\frac{1}{X_p}\\mathbb{E}_{x\\sim q}[f(x)w(x)] \\\\\n&= \\frac{\\mathbb{E}_{x\\sim q}[f(x)w(x)]}{\\mathbb{E}_{x\\sim q}[w(x)]} \\\\\n&\\approx \\frac{\\sum_{i=1}^Nf(x^{(i)})w(x^{(i)})}{\\sum_{i=1}^Nw(x^{(i)})} \\\\\n&= \\sum_{i=1}^Nf(x^{(i)})\\tilde{w}(x^{(i)})\n\\end{align*}\n$$\n\nwhere $\\tilde{w}(x^{(i)})$ is the normalized importance weight.\n\nDrawback: the normalized importance sampling estimator is biased. When $N=1$,\n\n$$\n\\mathbb{E}_{x\\sim q}\\left[\\hat{\\mathbb{E}}_{x\\sim p}[f(x)]\\right]\n=\\mathbb{E}_{x\\sim q}\\left[\\frac{f(x)w(x)}{w(x)}\\right]\n=\\mathbb{E}_{x\\sim q}[f(x)]\\neq \\mathbb{E}_{x\\sim p}[f(x)]\n$$\n\nFortunately, because the numerator and denominator are both unbiased, the normalized importance sampling estimator remains asymptotically unbiased.\n\n### Example: Approximate posterior probability $p(x_i|e)$ (discrete space)\n\nDenote posterior probabilities $p_e(z)\\triangleq p(z|e)$ and $\\tilde{p}_e(z)\\triangleq p(z,e)$. We then have\n\n$$\n\\begin{align*}\np(x_i|e)\n&= p_e(x_i) \\\\\n&= \\sum_z\\mathbb{1}_{x_i}(z)p_e(z) \\\\\n&= \\mathbb{E}_{z\\sim p_e}[\\mathbb{1}_{x_i}(z)]  \\\\\n&\\approx \\sum_{i=1}^N\\mathbb{1}_{x_i}(z)\\tilde{w}(z^{(i)})\n\\end{align*}\n$$\n\nwhere the unnormalised importance weight is $w(z)\\triangleq \\tilde{p}_e(z)/q(z)=p(z,e)/q(z)$.\n\n## Markov chain Monte Carlo (MCMC)\n\nMCMC is a strategy for generating samples $x^{(i)}$ while exploring the state space $\\mathcal{X}$ using a Markov chain mechanism. This mechanism is consturcted so that the chain spends more time in the most important regions, i.e. samples $x^{(i)}$ mimic samples drawn from target distribution $\\tilde{p}(x)$.\n\n### Markov Chain on finite state spaces\n\nConsider a discrete-time stochastic process $x^{(i)}$ is called a Markov chain if it satisfies the _Markov assumption_:\n\n$$p(x^{(i)}|x^{(i-1)}, ..., x^{1})=T(x^{(i)}|x^{(i-1)})$$\n\nThe chain is _homogenous_ if $T\\triangleq T(x^{(i)}|x^{(i-1)})$ remains invariant for all $i$, with $\\sum_{x^{(i)}}T(x^{(i)}|x^{(i-1)})=1$ for any $i$.\n\nIf the initial state $x^{(0)}$ is drawn from probability vector $p(x^{(0)})$, we may represent the probability $p(x^{(t)})$ of ending up in each state after $t$ steps as\n\n$$p(x^{(t)})=T^tp(x^{(0)})$$\n\nThe _stationary distribution_ of the Markov chain is the limit $\\pi=\\lim_{t\\rightarrow\\infty}p(x^{(t)})$ if it exists.\n\n### Existence of a stationary distribution\n\nThe high-level idea of MCMC will be to construct a Markov chain whose states will be joint assignments to the variables in the model and whose stationary distribution will equal the model probability $p$.\n\nTwo sufficient conditions on $T$ for finite-state Markov chain to have a stationary distribution:\n\n- _Irreducibility_: It is possible to get from any state $x$ to any other state $x'$ with a positive probability in a finite number of steps. In other words, $T$ cannot be reduced to separate smaller matrices annd the transition graph is connected.\n- _Aperiodicity_: The train should not be trapped in cycles. In other words, it is possible to return to any state at any time, i.e., there exists an $n$ such that for all $k$ and all $n'\\geq n$, $p(x^{(n')}=k|x^{(n)}=k)>0$.\n\nIn the case of continuuous variables, the Markov chain must be _ergodic_.\n\nA sufficient (but not necessary) condition that a particular distribution $p(x)$ is a stationary distribution is the following _detailed balance_ (reversibility) condition:\n\n$$p(x^{(i)})T(x^{(i-1)}|x^{(i)})=p(x^{(i-1)})T(x^{(i)}|x^{(i-1)}) \\enspace \\forall x^{(i-1)}$$\n\nProof: summing both sides over $x^{(i-1)}$ gives us\n\n$$p(x^{(i)})=\\sum_{x^{(i-1)}}p(x^{(i-1)})T(x^{(i)}|x^{(i-1)})$$\n\nMCMC samplers are irreducible and aperiodic Markov chains that have the target distribution as the invariant distribution. One way to design these samplers is to ensure that detailed balance is satisfied.\n\n### MCMC algorithms\n\nAt a high level, MCMC algorithms will have the following structure. They take as argument a transition operator $T$ specifying a Markov chain whose stationary distribution is $p$ (unnormalised), and an initial assignment $x_0$ to the variables of $p$. An MCMC algorithm then perform the following steps.\n\n1. Run the Markov chain from $x_0$ for $B$ _burn-in_ steps.\n2. Run the Markov chain for $N$ _sampling_ steps and collect all the states that it visits.\n\nAssuming $B$ is sufficiently large, the latter collection of states will form samples from $p$. We may then use these samples for Monte Carlo integration (or in importance sampling). We may also use them to:\n\n- produce Monte Carlo estimates of marginal probabilities,\n- perform MAP inference by take the sample with the highest probability\n\n### Metropolis-Hastings algorithm\n\nThe Metropolis-Hastings (MH) algorithm is our first way to construct Markov chains within MCMC. The MH method constructs a transition operator $T$ from two components:\n\n- A transition kernel $q(x^*|x)$, that is our proposal distribution.\n- An acceptance probability for moving towards candidate value $x^*$ sampled from $q(x^*|x)$\n\n$$\\mathcal{A}(x^*, x)=\\min\\left\\{1, \\frac{p(x^*)q(x|x^*)}{p(x)q(x^*|x)}\\right\\}$$\n\n---\n\n**Rejection Sampling Algorithm**\n\n1. Initialise $x^{(0)}$.\n2. For $i=0$ to $N-1$\n   - Sample $u\\sim U(0, 1)$.\n   - Sample $x^*\\sim q(x^*|x^{(i)})$.\n   - If $u<\\mathcal{A}(x^*, x^{(i)})$, assign $x^{(i+1)}=x^*$; else, $x^{(i+1)}=x^{(i)}$\n\n---\n\nNotice that the acceptance probability encourages us to move towards more likely points in the distribution; when $q$ suggests that we move into a low-probability region, we follow that move only a certain fraction of the time.\n\nIn practice, the distribution $q$ is taken to be something simple, like a Gaussian centered at $x$ if we are dealing with continuous variables.\n\n#### Fact 1: The MH algo admits $p$ as a stationary distribution.\n\nThis means that the MH algo will eventually produce samples from their stationary distribution, which is $p$ by construction.\n\nWe shall prove that $p$ satisfies the detailed balance condition w.r.t the MH Markov chain. The transition kernel for the MH algorithm is\n\n$$T(x^{(i+1)}|x^{(i)})=q(x^{(i+1)}|x^{(i)})\\mathcal{A}(x^{(i+1)}, x^{(i)})+\\mathcal{1}_{x^{(i)}}(x^{(i+1)})r(x^{(i)})$$\n\nwhere $r(x^{(i)})$ is the term associated with rejection\n\n$$r(x^{(i)})=\\int_{\\mathcal{X}}q(x^*|x^{(i)})(1-\\mathcal{A}(x^*, x^{(i)}))dx^*$$\n\nCase 1: $x^{(i+1)}=x^{(i)}$ (candidate sample is rejected). Then the detailed balance condition is trivially evident.\n\nCase 2: $x^{(i+1)}\\neq x^{(i)}$. We have,\n\n$$\n\\begin{align*}\nT(x^{(i+1)}|x^{(i)})\n&= q(x^{(i+1)}|x^{(i)})\\min\\left\\{1, \\frac{p(x^{(i+1)})q(x^{(i)}|x^{(i+1)})}{p(x^{(i)})q(x^{(i+1)}|x^{(i)})}\\right\\} \\\\\n&= \\frac{1}{p(x^{(i)})}\\min\\left\\{p(x^{(i)})q(x^{(i+1)}|x^{(i)}), p(x^{(i+1)})q(x^{(i)}|x^{(i+1)})\\right\\} \\\\\n\\implies T(x^{(i+1)}|x^{(i)})p(x^{(i)}) &= M(x^{(i+1)},x^{(i)})\n\\end{align*}\n$$\n\nwhere $M(x^{(i+1)},x^{(i)})=\\min\\left\\{p(x^{(i)})q(x^{(i+1)}|x^{(i)}), p(x^{(i+1)})q(x^{(i)}|x^{(i+1)})\\right\\}$. Likewhise, it can be shown that $T(x^{(i)}|x^{(i+1)})p(x^{(i+1)}) = M(x^{(i)},x^{(i+1)})$.\n\nObserve that $M$ is symmetric i.e. $M(x^{(i+1)},x^{(i)})=M(x^{(i)},x^{(i+1)})$. The detailed balance condition then follows.\n\nTo ensure that the MH algo converges:\n\n- Aperiodicity: no additional conditions since rejection is allowed.\n- Irreducibility: the support of $q$ must include the support of $p$.\n\n#### Fact 3: The normalising constant of the target distribution is not required.\n\n#### Fact 4: Success of failure of the algo often hinges on the choice of $q$.\n\n- If the proposal is too narrow, only one mode of $p$ might be visited.\n- If the proposal is too wide, the rejection rate can be very high, resulting in high correlations.\n- If all the modes are visited while the acceptance probability is high, the chain is said to \"mix\" well.\n\nBelow shows approximations obtained using the MH algorithm with three Gaussian proposal distributions of different variances.\n![Metropolis-Hastings algorithm](mh-algo.png)\n\n### Independent sampler\n\nIn the independent sampler, the proposal is independent of the current state, $q(x^*|x^{(i)})=q(x^*)$. Hence the acceptance probability is\n\n$$\\mathcal{A}(x^*, x^{(i)})=\\min\\left\\{1, \\frac{p(x^*)q(x^{(i)})}{p(x^{i})q(x^*)}\\right\\}=\\min\\left\\{1, \\frac{w(x^*)}{w(x^{(i)})}\\right\\}$$\n\nThis algo is close to importance sampling, but now the samples are correlated since they result from comparing one sample to the other.\n\n### Metropolis algorithm\n\nThe Metropolis algo assumes a symmetric random walk proposal $q(x^*|x^{(i)})=q(x^{(i)}|x^*)$ e.g. isotropic Gaussian. The acceptance ratio simplifies to\n\n$$\\mathcal{A}(x^*, x^{(i)})=\\min\\left\\{1, \\frac{p(x^*)}{p(x^{i})}\\right\\}$$\n\n### Gibbs sampler\n\nSuppose we have an $n$-dimensional vector $x$ and the expressions for the full conditionals $p(x_j|x_1, ..., x_{j-1}, x_{j+1}, ..., x_n)=p(x_j|x_{-j})$. Consider the following proposal\n\n$$\nq(x^*|x^{(i)})=\n\\begin{cases}\np(x_j^*|x_{-j}^{(i)}) & \\text{if } x_j^*=x_j^{(i)} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\n\nThe acceptance probability will be\n\n$$\n\\begin{align*}\n\\mathcal{A}(x^{(i)}, x^*)\n&= \\min\\left\\{1, \\frac{p(x^*)q(x^{(i)}|x^*)}{p(x^{(i)})q(x^*|x^{(i)})}\\right\\} \\\\\n&= \\min\\left\\{1, \\frac{p(x_j^*)p(x_{-j}^*)p(x_j^{(i)}|x_{-j}^*)}{p(x_j^{(i)})p(x_{-j}^{(i)})p(x_j^*|x_{-j}^{(i)})}\\right\\} \\\\\n&= 1\n\\end{align*}\n$$\n\n---\n\n**Gibbs sampling algo**\n\n1. Initialise $x_{0, 1:n}$.\n2. For $i=0$ to $N-1$,\n   - Sample $x_1^{(i+1)}\\sim p(x_1|x_2^{(i)}, x_3^{(i)}, ..., x_n^{(i)})$.\n   - Sample $x_2^{(i+1)}\\sim p(x_2|x_1^{(i+1)}, x_3^{(i)}, ..., x_n^{(i)})$.\n   - $\\vdots$\n   - Sample $x_j^{(i+1)}\\sim p(x_j|x_1^{(i+1)}, ..., x_{j-1}^{(i+1)}, x_{j+1}^{(i)}, ..., x_n^{(i)})$.\n   - $\\vdots$\n   - Sample $x_n^{(i+1)}\\sim p(x_n|x_1^{(i+1)}, x_2^{(i+1)}, ..., x_{n-1}^{(i+1)})$.\n\n---\n\nFor graphical models, full conditionals reduces to conditonals on Markov blankets i.e. $p(x_j|x_{-j})=p(x_j|\\text{MB}(x_j))$\n\n### Monte carlo EM\n\n## Reference materials\n\n- Andrieu, C., de Freitas, N., Doucet A., Jordan, M. I. \"An Introduction to MCMC for Machine Learning.\" _Machine Learning_, 50, 5-43, 2003. Accessed Nov 2, 2021. https://link.springer.com/content/pdf/10.1023/A:1020281327116.pdf.\n- Kuleshov, V. and Ermon, S. \"Sample methods.\" _cs228-notes_. Accessed Nov 2, 2021. https://ermongroup.github.io/cs228-notes/inference/sampling/.\n- Owen, A. \"Importance sampling.\" _Monte Carlo theory, methods and examples_. Accessed Nov 2, 2021. https://statweb.stanford.edu/~owen/mc/Ch-var-is.pdf.\n- Mauser, K. \"Why does the Metropolis-Hastings procedure satisfy the detailed balance criterion?\" _Kris Hauser_. Accessed Nov 2, 2021. https://people.duke.edu/~kh269/teaching/notes/MetropolisExplanation.pdf.\n- Bishop, C. \"Sample methods.\" _Pattern Recognition and Machine Learning_.\n"}},"__N_SSG":true}